{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97267,"databundleVersionId":11591267,"sourceType":"competition"},{"sourceId":11200531,"sourceType":"datasetVersion","datasetId":6993215}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:09:35.179701Z","iopub.execute_input":"2025-04-03T06:09:35.179984Z","iopub.status.idle":"2025-04-03T06:09:35.183876Z","shell.execute_reply.started":"2025-04-03T06:09:35.179961Z","shell.execute_reply":"2025-04-03T06:09:35.182898Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# ✅ Define Paths (Modify these if needed)\ntrain_data_path = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Training_Dataset_TB/\"\n\ntest_data_path = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Test_Dataset_TB\"\ncsv_path = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\" \n\n# ✅ Load the CSV File\ndf = pd.read_csv(csv_path)\n\n# ✅ Check the first few rows\nprint(df.head())\n\n# ✅ Check if images exist in the folder\nmissing_images = [img for img in df[\"ID\"] if not os.path.exists(os.path.join(train_data_path, img))]\n\nprint(f\"❌ Missing images: {len(missing_images)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:09:39.286744Z","iopub.execute_input":"2025-04-03T06:09:39.287045Z","iopub.status.idle":"2025-04-03T06:09:42.735267Z","shell.execute_reply.started":"2025-04-03T06:09:39.287022Z","shell.execute_reply":"2025-04-03T06:09:42.734432Z"}},"outputs":[{"name":"stdout","text":"                  ID  Target\n0  TB_train_0001.png       1\n1  TB_train_0002.png       0\n2  TB_train_0003.png       0\n3  TB_train_0004.png       0\n4  TB_train_0005.png       0\n❌ Missing images: 0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#understanding csv file columns \nimport pandas as pd\n\n# ✅ Load the CSV file\ncsv_path = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\"\nlabels_df = pd.read_csv(csv_path)\n\n# ✅ Print column names\nprint(labels_df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:09:46.067597Z","iopub.execute_input":"2025-04-03T06:09:46.067929Z","iopub.status.idle":"2025-04-03T06:09:46.077102Z","shell.execute_reply.started":"2025-04-03T06:09:46.067889Z","shell.execute_reply":"2025-04-03T06:09:46.076153Z"}},"outputs":[{"name":"stdout","text":"Index(['ID', 'Target'], dtype='object')\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\n# ✅ Load labels from CSV\nlabels_df = pd.read_csv(\"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\")  # Ensure correct path\n# ✅ Adjust column names based on the CSV file\nlabels_dict = dict(zip(labels_df[\"ID\"], labels_df[\"Target\"]))  # Use actual column names\n  # Convert to dictionary\n\nclass TuberculosisDataset(Dataset):\n    def __init__(self, image_folder, transform=None):\n        self.image_folder = image_folder\n        self.image_files = os.listdir(image_folder)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.image_folder, img_name)\n\n        # ✅ Open image\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # ✅ Get label from dictionary\n        label = labels_dict.get(img_name, -1)  # Default to -1 if not found\n\n        # ✅ Apply transformations\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\nprint(\"✅ Labels assigned successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:09:48.683624Z","iopub.execute_input":"2025-04-03T06:09:48.683952Z","iopub.status.idle":"2025-04-03T06:09:51.940147Z","shell.execute_reply.started":"2025-04-03T06:09:48.683922Z","shell.execute_reply":"2025-04-03T06:09:51.939454Z"}},"outputs":[{"name":"stdout","text":"✅ Labels assigned successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#printing images to check if labeled properly\n# ✅ Print a few label mappings\nfor i, (img_name, label) in enumerate(labels_dict.items()):\n    print(f\"Image: {img_name}, Label: {label}\")\n    if i == 9:  # Print only first 10 entries\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:09:55.529367Z","iopub.execute_input":"2025-04-03T06:09:55.529653Z","iopub.status.idle":"2025-04-03T06:09:55.534791Z","shell.execute_reply.started":"2025-04-03T06:09:55.529630Z","shell.execute_reply":"2025-04-03T06:09:55.533472Z"}},"outputs":[{"name":"stdout","text":"Image: TB_train_0001.png, Label: 1\nImage: TB_train_0002.png, Label: 0\nImage: TB_train_0003.png, Label: 0\nImage: TB_train_0004.png, Label: 0\nImage: TB_train_0005.png, Label: 0\nImage: TB_train_0006.png, Label: 0\nImage: TB_train_0007.png, Label: 1\nImage: TB_train_0008.png, Label: 0\nImage: TB_train_0009.png, Label: 0\nImage: TB_train_0010.png, Label: 0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#checking for missing labels in training dataset\nimport os\n\n# ✅ Get all image filenames\nimage_folder = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Training_Dataset_TB\"\nimage_filenames = set(os.listdir(image_folder))\n\n# ✅ Check for missing labels\nmissing_labels = [img for img in image_filenames if img not in labels_dict]\nif missing_labels:\n    print(f\"⚠️ {len(missing_labels)} images are missing labels!\")\n    print(missing_labels[:10])  # Print first 10 missing images\nelse:\n    print(\"✅ All images have labels.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:09:58.352729Z","iopub.execute_input":"2025-04-03T06:09:58.353033Z","iopub.status.idle":"2025-04-03T06:09:58.380541Z","shell.execute_reply.started":"2025-04-03T06:09:58.353008Z","shell.execute_reply":"2025-04-03T06:09:58.379682Z"}},"outputs":[{"name":"stdout","text":"✅ All images have labels.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#checking for missing labels in test dataset\nimport os\n\n# ✅ Get all image filenames\nimage_folder = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Test_Dataset_TB\"\nimage_filenames = set(os.listdir(image_folder))\n\n# ✅ Check for missing labels\nmissing_labels = [img for img in image_filenames if img not in labels_dict]\nif missing_labels:\n    print(f\"⚠️ {len(missing_labels)} images are missing labels!\")\n    print(missing_labels[:10])  # Print first 10 missing images\nelse:\n    print(\"✅ All images have labels.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:00.696632Z","iopub.execute_input":"2025-04-03T06:10:00.696946Z","iopub.status.idle":"2025-04-03T06:10:00.717197Z","shell.execute_reply.started":"2025-04-03T06:10:00.696918Z","shell.execute_reply":"2025-04-03T06:10:00.716560Z"}},"outputs":[{"name":"stdout","text":"⚠️ 1260 images are missing labels!\n['TB_test_0109.png', 'TB_test_0979.png', 'TB_test_0509.png', 'TB_test_0398.png', 'TB_test_1118.png', 'TB_test_0122.png', 'TB_test_0918.png', 'TB_test_0333.png', 'TB_test_0982.png', 'TB_test_0894.png']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# ✅ Paths\ntrain_csv_path = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\"\ntrain_dir = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Training_Dataset_TB\"\ntest_dir = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Test_Dataset_TB\"\n\n# ✅ Load train labels\ntrain_df = pd.read_csv(train_csv_path)\n\n# ✅ Extract train image names & labels as a dictionary\ntrain_labels_dict = dict(zip(train_df[\"ID\"], train_df[\"Target\"]))\n\n# ✅ Get all test images\ntest_images = os.listdir(test_dir)\n\n# ✅ Assign -1 as label for test images\ntest_labels_dict = {img: -1 for img in test_images}\n\n# ✅ Combine Train + Test into One DataFrame\ncombined_labels_dict = {**train_labels_dict, **test_labels_dict}\ncombined_df = pd.DataFrame(list(combined_labels_dict.items()), columns=[\"image_name\", \"label\"])\n\n# ✅ Save Updated CSV\nupdated_csv_path = \"/kaggle/working/TB_combined_labels.csv\"\ncombined_df.to_csv(updated_csv_path, index=False)\n\nprint(f\"✅ Updated labels CSV saved at: {updated_csv_path}\")\nprint(f\"Total Labeled Data: {len(combined_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:03.596418Z","iopub.execute_input":"2025-04-03T06:10:03.596704Z","iopub.status.idle":"2025-04-03T06:10:03.620029Z","shell.execute_reply.started":"2025-04-03T06:10:03.596683Z","shell.execute_reply":"2025-04-03T06:10:03.619148Z"}},"outputs":[{"name":"stdout","text":"✅ Updated labels CSV saved at: /kaggle/working/TB_combined_labels.csv\nTotal Labeled Data: 4200\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#new approach\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# ✅ Load dataset\ncsv_path = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\"\ndf = pd.read_csv(csv_path)\n\n# ✅ Split into 80% Training & 20% Validation\ntrain_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"Target\"], random_state=42)\n\nprint(f\"✅ Training Samples: {len(train_df)}, Validation Samples: {len(val_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:06.571376Z","iopub.execute_input":"2025-04-03T06:10:06.571685Z","iopub.status.idle":"2025-04-03T06:10:07.173678Z","shell.execute_reply.started":"2025-04-03T06:10:06.571659Z","shell.execute_reply":"2025-04-03T06:10:07.172904Z"}},"outputs":[{"name":"stdout","text":"✅ Training Samples: 2352, Validation Samples: 588\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\n\n# ✅ Apply CLAHE for Image Enhancement\ndef apply_clahe(image):\n    img_array = np.array(image.convert(\"L\"))  # Convert to grayscale\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    clahe_img = clahe.apply(img_array)  # Apply CLAHE\n    return Image.fromarray(clahe_img)\n\n# ✅ Define Image Transformations (Same for Train & Validation)\nimage_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),  \n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n\n# ✅ Define Tuberculosis Dataset Class\nclass TuberculosisDataset(Dataset):\n    def __init__(self, data, image_folder, transform=None, is_test=False):\n        self.data = data\n        self.image_folder = image_folder\n        self.transform = transform\n        self.is_test = is_test  # 🔥 Important flag for test set\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        try:\n            img_name = self.data.iloc[idx, 0]  # ✅ Get actual image filename\n            \n            # ✅ Handle Test Set (No Labels)\n            if self.is_test:\n                label = -1  # Dummy label for test images\n            else:\n                label = self.data.iloc[idx, 1]  # Get label normally\n            \n            img_path = os.path.join(self.image_folder, img_name)\n    \n            if not os.path.exists(img_path):\n                print(f\"⚠️ Warning: {img_name} not found! Skipping...\")\n                return self.__getitem__((idx + 1) % len(self.data))  # Skip to next image\n    \n            image = Image.open(img_path).convert(\"RGB\")\n            if self.transform:\n                image = self.transform(image)\n            \n            # ✅ Return correctly formatted output\n            return (image, img_name) if self.is_test else (image, torch.tensor(label, dtype=torch.long))\n\n        except IndexError:\n            print(f\"❌ Error: Index {idx} out of bounds. Skipping...\")\n            return None\n\n\n# ✅ Define Paths\ncsv_path = \"/kaggle/working/TB_combined_labels.csv\"  # Updated CSV\nimage_folder = \"/kaggle/input/tb-data/tbdata/Training_Dataset_TB\"  # Training images path\n\n# ✅ Load CSV and Remove Test Data (-1 Labels)\ndf = pd.read_csv(csv_path)\ndf = df[df[\"label\"] != -1]  # Remove test images\n\n# ✅ Split into 80% Train & 20% Validation\ntrain_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n\n# ✅ Balance TB Cases by Up-Sampling\ntb_samples = train_df[train_df[\"label\"] == 1]  # Extract TB samples\ntrain_df = pd.concat([train_df, tb_samples.sample(len(train_df[train_df[\"label\"] == 0]), replace=True)])\n\nprint(f\"✅ Training Samples: {len(train_df)}, Validation Samples: {len(val_df)}\")\n\n# ✅ Create Dataset Objects\ntrain_dataset = TuberculosisDataset(train_df, image_folder, transform=image_transforms)\nval_dataset = TuberculosisDataset(val_df, image_folder, transform=image_transforms)\n\n# ✅ Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n\nprint(\"✅ DataLoaders Ready: Train =\", len(train_loader), \", Validation =\", len(val_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:10.019053Z","iopub.execute_input":"2025-04-03T06:10:10.019399Z","iopub.status.idle":"2025-04-03T06:10:12.735419Z","shell.execute_reply.started":"2025-04-03T06:10:10.019369Z","shell.execute_reply":"2025-04-03T06:10:12.734567Z"}},"outputs":[{"name":"stdout","text":"✅ Training Samples: 4312, Validation Samples: 588\n✅ DataLoaders Ready: Train = 270 , Validation = 37\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"test_image_folder = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Test_Dataset_TB\"  # ✅ Test images path\n\n# ✅ Load test image names (No labels)\ntest_df = pd.DataFrame({\"ID\": os.listdir(test_image_folder)})\n\n# ✅ Create Test Dataset (Ensure is_test=True)\ntest_dataset = TuberculosisDataset(test_df, test_image_folder, transform=image_transforms, is_test=True)\n\n# ✅ Create DataLoader\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n\nprint(f\"✅ Reloaded test_loader with {len(test_dataset)} samples.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:16.558919Z","iopub.execute_input":"2025-04-03T06:10:16.559203Z","iopub.status.idle":"2025-04-03T06:10:16.565326Z","shell.execute_reply.started":"2025-04-03T06:10:16.559181Z","shell.execute_reply":"2025-04-03T06:10:16.564559Z"}},"outputs":[{"name":"stdout","text":"✅ Reloaded test_loader with 1260 samples.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ✅ Load CSV for Splitting Train and Test\ndf = pd.read_csv(\"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\")\n\n# ✅ Separate Train and Test Data\ntrain_df = df[df[\"Target\"] != -1]  # Only labeled data\ntest_df = df[df[\"Target\"] == -1]  # Unlabeled test data\n\nprint(f\"✅ Training samples: {len(train_df)}, Test samples: {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:22.709198Z","iopub.execute_input":"2025-04-03T06:10:22.709514Z","iopub.status.idle":"2025-04-03T06:10:22.719373Z","shell.execute_reply.started":"2025-04-03T06:10:22.709491Z","shell.execute_reply":"2025-04-03T06:10:22.718639Z"}},"outputs":[{"name":"stdout","text":"✅ Training samples: 2940, Test samples: 0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(train_df.columns)  # Check the column names\ntrain_df = pd.read_csv(\"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\", header=0)  # Ensure first row is used as column names\nprint(train_df.head()) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:24.988646Z","iopub.execute_input":"2025-04-03T06:10:24.988951Z","iopub.status.idle":"2025-04-03T06:10:24.998647Z","shell.execute_reply.started":"2025-04-03T06:10:24.988927Z","shell.execute_reply":"2025-04-03T06:10:24.997955Z"}},"outputs":[{"name":"stdout","text":"Index(['ID', 'Target'], dtype='object')\n                  ID  Target\n0  TB_train_0001.png       1\n1  TB_train_0002.png       0\n2  TB_train_0003.png       0\n3  TB_train_0004.png       0\n4  TB_train_0005.png       0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\n\n# ✅ Load the CSV file\ncsv_path = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\"\ntrain_df = pd.read_csv(csv_path)\n\n# ✅ Check if 'Target' column exists\nprint(\"Columns in train_df:\", train_df.columns)\n\n# ✅ Compute Class Weights\nclass_counts = train_df[\"Target\"].value_counts().sort_index().to_numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:27.154616Z","iopub.execute_input":"2025-04-03T06:10:27.154919Z","iopub.status.idle":"2025-04-03T06:10:27.169337Z","shell.execute_reply.started":"2025-04-03T06:10:27.154897Z","shell.execute_reply":"2025-04-03T06:10:27.168614Z"}},"outputs":[{"name":"stdout","text":"Columns in train_df: Index(['ID', 'Target'], dtype='object')\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":" import torch\n\n# ✅ Set Device (Use Multi-GPU If Available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"✅ Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:29.533920Z","iopub.execute_input":"2025-04-03T06:10:29.534381Z","iopub.status.idle":"2025-04-03T06:10:29.600900Z","shell.execute_reply.started":"2025-04-03T06:10:29.534343Z","shell.execute_reply":"2025-04-03T06:10:29.599927Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import os\nimport torch\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Debugging\n\n# ✅ Compute Class Weights\nclass_counts = train_df[\"Target\"].value_counts().sort_index().to_numpy()  # [Normal, TB]\n\n# ✅ Fix Misalignment Issue\nwith torch.cuda.amp.autocast(enabled=False):  \n    class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float32)\n    class_weights = class_weights.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:35.856031Z","iopub.execute_input":"2025-04-03T06:10:35.856374Z","iopub.status.idle":"2025-04-03T06:10:35.862874Z","shell.execute_reply.started":"2025-04-03T06:10:35.856344Z","shell.execute_reply":"2025-04-03T06:10:35.862002Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-18-33c207027206>:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=False):\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# ✅ Ensure CSV is Loaded\ncsv_path = \"/kaggle/working/TB_combined_labels.csv\"\ndf = pd.read_csv(csv_path)\n\n# ✅ Remove Test Data (-1 Labels)\ndf = df[df[\"label\"] != -1]  # Exclude test samples\n\n# ✅ Split into 80% Train & 20% Validation (Stratified)\ntrain_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n\nprint(f\"✅ Training Samples: {len(train_df)}, Validation Samples: {len(val_df)}\")\n\n# ✅ Define Image Folder\nimage_folder = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Training_Dataset_TB\"\n\n# ✅ Create Train & Validation Datasets\ntrain_dataset = TuberculosisDataset(train_df, image_folder, transform=image_transforms)\nval_dataset = TuberculosisDataset(val_df, image_folder, transform=image_transforms)\n\nprint(\"✅ Train & Validation Datasets Created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:38.304957Z","iopub.execute_input":"2025-04-03T06:10:38.305290Z","iopub.status.idle":"2025-04-03T06:10:38.318138Z","shell.execute_reply.started":"2025-04-03T06:10:38.305261Z","shell.execute_reply":"2025-04-03T06:10:38.317299Z"}},"outputs":[{"name":"stdout","text":"✅ Training Samples: 2352, Validation Samples: 588\n✅ Train & Validation Datasets Created!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ✅ Import necessary libraries\nimport torch\nimport torchvision.models as models\nfrom torch.utils.data import WeightedRandomSampler, DataLoader\nfrom torchvision.models import EfficientNet_B0_Weights  # Import weight class\n\n# ✅ Set Device (Use GPUs If Available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"✅ Using device: {device}\")\n\n# ✅ Define Model (EfficientNet-B0 with Pretrained Weights)\nmodel = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)  # Updated syntax\nmodel.to(device)  # ✅ Move model to GPU\nprint(\"✅ Model moved to device successfully!\")\n\n# ✅ Ensure train_df exists before computing class weights\nif \"train_df\" not in locals():\n    raise ValueError(\"❌ ERROR: train_df is not defined!\")\n\n# ✅ Compute Class Weights for Imbalance Handling\nclass_counts = train_df[\"label\"].value_counts().sort_index().to_numpy()  # Use correct column name\nclass_weights = torch.tensor(1.0 / class_counts, dtype=torch.float32).to(device)\n\n# ✅ Ensure train_dataset & val_dataset Exist\nif \"train_dataset\" not in locals() or \"val_dataset\" not in locals():\n    raise ValueError(\"❌ ERROR: train_dataset or val_dataset is not defined!\")\n\n# ✅ Create Weighted Sampler (For Balanced Training)\nsample_weights = [class_weights[label] for label in train_df[\"label\"].tolist()]\nsampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n\n# ✅ Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n\nprint(f\"✅ DataLoaders Created: Train={len(train_loader)}, Validation={len(val_loader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:41.234916Z","iopub.execute_input":"2025-04-03T06:10:41.235252Z","iopub.status.idle":"2025-04-03T06:10:41.849294Z","shell.execute_reply.started":"2025-04-03T06:10:41.235226Z","shell.execute_reply":"2025-04-03T06:10:41.848391Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","output_type":"stream"},{"name":"stdout","text":"✅ Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20.5M/20.5M [00:00<00:00, 74.3MB/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Model moved to device successfully!\n✅ DataLoaders Created: Train=147, Validation=37\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import timm\nimport torch\nimport torch.nn as nn\n\n# ✅ Define Hybrid Model (EfficientNet-B3 + ViT)\nclass HybridTBModel(nn.Module):\n    def __init__(self, num_classes=2):\n        super().__init__()\n\n        # ✅ EfficientNet-B3 as CNN Backbone\n        self.efficientnet = timm.create_model(\"efficientnet_b3\", pretrained=True, num_classes=0)  # Remove classifier\n        self.efficientnet_out = self.efficientnet.num_features  # Get feature size\n\n        # ✅ Vision Transformer as Transformer Backbone\n        self.vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)  # Remove classifier\n        self.vit_out = self.vit.num_features  # Get feature size\n\n        # ✅ Final Classifier (Combining Features)\n        self.fc = nn.Linear(self.efficientnet_out + self.vit_out, num_classes)\n\n    def forward(self, x):\n        cnn_features = self.efficientnet(x)  # Extract CNN features\n        transformer_features = self.vit(x)  # Extract Transformer features\n        combined = torch.cat((cnn_features, transformer_features), dim=1)  # Concatenate features\n        return self.fc(combined)  # Final classification\n\n# ✅ Initialize Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = HybridTBModel(num_classes=2).to(device)\n\n# ✅ Enable Multi-GPU Training (If Available)\nif torch.cuda.device_count() > 1:\n    print(f\"⚡ Using {torch.cuda.device_count()} GPUs for training!\")\n    model = nn.DataParallel(model).to(device)\n\nprint(\"✅ Hybrid Model initialized successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:10:51.359910Z","iopub.execute_input":"2025-04-03T06:10:51.360196Z","iopub.status.idle":"2025-04-03T06:10:58.095923Z","shell.execute_reply.started":"2025-04-03T06:10:51.360174Z","shell.execute_reply":"2025-04-03T06:10:58.095106Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f77034378b044814a9eff6aa3bff6114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3207965ea04049fc9b54828c1af4ba8d"}},"metadata":{}},{"name":"stdout","text":"✅ Hybrid Model initialized successfully!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.amp import GradScaler\n\n# ✅ Ensure class_weights is on the correct device\nclass_weights = class_weights.to(device)\n\n# ✅ Define Optimizer & Learning Rate\noptimizer = optim.AdamW(\n    model.parameters(), \n    lr=0.00005,  # Lower LR for ViT stability\n    betas=(0.9, 0.999),  # Standard AdamW parameters\n    weight_decay=1e-4  # Helps prevent overfitting\n)\n\n# ✅ Use Weighted Loss for Class Imbalance\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n\n# ✅ Enable Mixed Precision Training\nscaler = GradScaler()  # No need for `device=\"cuda\"`\n\nprint(\"✅ Optimizer & Loss defined correctly!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:11:01.538923Z","iopub.execute_input":"2025-04-03T06:11:01.539241Z","iopub.status.idle":"2025-04-03T06:11:01.547157Z","shell.execute_reply.started":"2025-04-03T06:11:01.539199Z","shell.execute_reply":"2025-04-03T06:11:01.546468Z"}},"outputs":[{"name":"stdout","text":"✅ Optimizer & Loss defined correctly!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# import pandas as pd\n# import os\n\n# # ✅ Define test folder path\n# test_image_folder = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Test_Dataset_TB\"\n\n# # ✅ Get list of test images\n# test_images = os.listdir(test_image_folder)\n\n# # ✅ Create `test_df` with image names (without labels)\n# test_df = pd.DataFrame({\"image_name\": test_images, \"label\": -1})  \n\n# print(f\"✅ Test DataFrame Shape: {test_df.shape}\")\n# print(test_df.head())  # Check first few rows\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T03:06:51.216388Z","iopub.execute_input":"2025-03-30T03:06:51.216779Z","iopub.status.idle":"2025-03-30T03:06:51.221033Z","shell.execute_reply.started":"2025-03-30T03:06:51.216749Z","shell.execute_reply":"2025-03-30T03:06:51.219868Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# print(train_df.columns)\n# # ✅ Create Test Dataset\n# test_dataset = TuberculosisDataset(test_df, test_image_folder, transform=test_transform)\n\n# # ✅ Create Test DataLoader\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# print(f\"✅ Test dataset loaded: {len(test_dataset)} samples\")\n# for images, img_names in test_loader:\n#     print(\"Batch Sample IDs:\", img_names[:10])  # Print first 10 image names\n#     break  # Only check one batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T00:16:16.900334Z","iopub.status.idle":"2025-03-30T00:16:16.900656Z","shell.execute_reply":"2025-03-30T00:16:16.900529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\n# ✅ Define Paths\ntrain_image_folder = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Training_Dataset_TB\"  # ✅ Define image folder\n\n# ✅ Load CSV file\ntrain_df = pd.read_csv(\"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\")\n\n# ✅ Check if 'Target' column exists\nif \"Target\" not in train_df.columns:\n    print(\"❌ ERROR: 'Target' column not found! Available columns:\", train_df.columns)\nelse:\n    print(\"✅ 'Target' column found! Proceeding with Upsampling & 80-20 split...\")\n\n    # ✅ Separate Normal & TB Cases\n    normal_cases = train_df[train_df[\"Target\"] == 0]\n    tb_cases = train_df[train_df[\"Target\"] == 1]\n\n    # ✅ Upsample TB Cases to Match Normal Cases\n    tb_upsampled = tb_cases.sample(len(normal_cases), replace=True, random_state=42)\n\n    # ✅ Combine Back\n    balanced_df = pd.concat([normal_cases, tb_upsampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n\n    print(f\"✅ Upsampled Data: {balanced_df['Target'].value_counts().to_dict()}\")\n\n    # ✅ Perform 80-20 Train-Validation Split\n    train_data, val_data = train_test_split(\n        balanced_df, test_size=0.2, stratify=balanced_df[\"Target\"], random_state=42\n    )\n\n    # ✅ Convert back to DataFrames\n    train_df = train_data.reset_index(drop=True)\n    val_df = val_data.reset_index(drop=True)\n\n    print(f\"✅ Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n\n    # ✅ Create Train & Validation Datasets\n    train_dataset = TuberculosisDataset(train_df, train_image_folder, transform=image_transforms)\n    val_dataset = TuberculosisDataset(val_df, train_image_folder, transform=image_transforms)\n\n    # ✅ Create Train & Validation Loaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n\n    print(\"✅ Train & Validation DataLoaders created successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:11:31.443515Z","iopub.execute_input":"2025-04-03T06:11:31.443812Z","iopub.status.idle":"2025-04-03T06:11:31.464330Z","shell.execute_reply.started":"2025-04-03T06:11:31.443788Z","shell.execute_reply":"2025-04-03T06:11:31.463513Z"}},"outputs":[{"name":"stdout","text":"✅ 'Target' column found! Proceeding with Upsampling & 80-20 split...\n✅ Upsampled Data: {1: 2450, 0: 2450}\n✅ Training samples: 3920, Validation samples: 980\n✅ Train & Validation DataLoaders created successfully!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"print(train_df.shape)  # Should return (num_samples, num_columns)\nprint(train_df.head())  # Display the first few rows\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:11:39.514547Z","iopub.execute_input":"2025-04-03T06:11:39.514827Z","iopub.status.idle":"2025-04-03T06:11:39.521041Z","shell.execute_reply.started":"2025-04-03T06:11:39.514805Z","shell.execute_reply":"2025-04-03T06:11:39.519993Z"}},"outputs":[{"name":"stdout","text":"(3920, 2)\n                  ID  Target\n0  TB_train_0181.png       0\n1  TB_train_1134.png       0\n2  TB_train_0724.png       0\n3  TB_train_2937.png       0\n4  TB_train_0455.png       0\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\n# ✅ Load CSV file\ntrain_df = pd.read_csv(\"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/TB_train.csv\")\n\n# ✅ Check if 'Target' column exists\nif \"Target\" not in train_df.columns:\n    print(\"❌ ERROR: 'Target' column not found! Available columns:\", train_df.columns)\nelse:\n    print(\"✅ 'Target' column found! Proceeding with 80-20 split...\")\n\n    # ✅ Perform 80-20 Train-Validation Split\n    train_data, val_data = train_test_split(\n        train_df, test_size=0.2, stratify=train_df[\"Target\"], random_state=42\n    )\n\n    # ✅ Convert back to DataFrames\n    train_df = train_data.reset_index(drop=True)\n    val_df = val_data.reset_index(drop=True)\n\n    print(f\"✅ Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n\n    # ✅ Create Training Dataset & DataLoader\n    train_dataset = TuberculosisDataset(train_df, train_image_folder, transform=image_transforms)\n    train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n\n    # ✅ Create Validation Dataset & DataLoader\n    val_dataset = TuberculosisDataset(val_df, train_image_folder, transform=image_transforms)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n    print(f\"✅ Reloaded DataLoader with {len(train_dataset)} training samples and {len(val_dataset)} validation samples.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:11:45.699745Z","iopub.execute_input":"2025-04-03T06:11:45.700093Z","iopub.status.idle":"2025-04-03T06:11:45.714292Z","shell.execute_reply.started":"2025-04-03T06:11:45.700061Z","shell.execute_reply":"2025-04-03T06:11:45.713365Z"}},"outputs":[{"name":"stdout","text":"✅ 'Target' column found! Proceeding with 80-20 split...\n✅ Training samples: 2352, Validation samples: 588\n✅ Reloaded DataLoader with 2352 training samples and 588 validation samples.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# ✅ List all images in the folder\nexisting_images = set(os.listdir(train_image_folder))\n\n# ✅ Filter train_df to only include images that exist\nmissing_images = [img for img in train_df[\"ID\"] if img not in existing_images]\n\nprint(f\"⚠️ Total Missing Images: {len(missing_images)}\")\nprint(\"Example Missing Images:\", missing_images[:10])  # Print first 10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:11:49.143072Z","iopub.execute_input":"2025-04-03T06:11:49.143411Z","iopub.status.idle":"2025-04-03T06:11:49.150459Z","shell.execute_reply.started":"2025-04-03T06:11:49.143382Z","shell.execute_reply":"2025-04-03T06:11:49.149698Z"}},"outputs":[{"name":"stdout","text":"⚠️ Total Missing Images: 0\nExample Missing Images: []\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(f\"✅ Training Dataset Size: {len(train_dataset)}\")\nprint(f\"✅ Validation Dataset Size: {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:11:52.125279Z","iopub.execute_input":"2025-04-03T06:11:52.125624Z","iopub.status.idle":"2025-04-03T06:11:52.131029Z","shell.execute_reply.started":"2025-04-03T06:11:52.125596Z","shell.execute_reply":"2025-04-03T06:11:52.130063Z"}},"outputs":[{"name":"stdout","text":"✅ Training Dataset Size: 2352\n✅ Validation Dataset Size: 588\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import torch\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.metrics import classification_report\n\n# ✅ Training Parameters\nnum_epochs = 10\npatience = 5  # Stop training if no improvement in 'patience' epochs\nbest_val_loss = float(\"inf\")  # Track best validation loss\nbest_val_acc = 0.0  # Track best validation accuracy\nearly_stopping_counter = 0  \n\n# ✅ Learning Rate Scheduler\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True)\n\nprint(\"🚀 Starting Training...\")\n\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    all_preds, all_labels = [], []\n\n    # ✅ Training Loop\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()  # Reset gradients\n        with torch.amp.autocast(device_type=\"cuda\"):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        # ✅ Backpropagation with Mixed Precision\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    train_loss = running_loss / len(train_loader)\n    print(f\"🟢 Epoch [{epoch+1}/{num_epochs}] - Training Loss: {train_loss:.4f}\")\n\n    # ✅ Training Performance Report\n    print(\"📊 Training Classification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=[\"Normal\", \"TB\"], zero_division=0))\n\n    # ✅ Validation Phase\n    model.eval()  # Set model to evaluation mode\n    val_preds, val_labels = [], []\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            with torch.amp.autocast(device_type=\"cuda\"):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_preds.extend(predicted.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    val_loss /= len(val_loader)\n    val_acc = (sum(np.array(val_preds) == np.array(val_labels)) / len(val_labels)) * 100\n\n    print(f\"🔵 Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n\n    # ✅ Validation Performance Report\n    print(\"📊 Validation Classification Report:\")\n    print(classification_report(val_labels, val_preds, target_names=[\"Normal\", \"TB\"], zero_division=0))\n\n    # ✅ Update Learning Rate Scheduler\n    scheduler.step(val_loss)\n\n    # ✅ Save Best Model (Based on Accuracy & Loss)\n    if val_loss < best_val_loss or val_acc > best_val_acc:\n        best_val_loss = min(val_loss, best_val_loss)\n        best_val_acc = max(val_acc, best_val_acc)\n        early_stopping_counter = 0\n        torch.save(model.state_dict(), \"efficientnet_vit_tb_best.pth\")  # Save best model\n        print(\"✅ Model improved & saved!\")\n    else:\n        early_stopping_counter += 1\n        print(f\"⚠️ Early stopping counter: {early_stopping_counter}/{patience}\")\n\n    if early_stopping_counter >= patience:\n        print(\"🛑 Early stopping triggered. Training stopped.\")\n        break\n\nprint(\"✅ Training Complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:11:55.024122Z","iopub.execute_input":"2025-04-03T06:11:55.024525Z","iopub.status.idle":"2025-04-03T06:32:21.000849Z","shell.execute_reply.started":"2025-04-03T06:11:55.024494Z","shell.execute_reply":"2025-04-03T06:32:20.999739Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"🚀 Starting Training...\n🟢 Epoch [1/10] - Training Loss: 0.1963\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.96      0.72      0.83      1197\n          TB       0.77      0.97      0.86      1155\n\n    accuracy                           0.84      2352\n   macro avg       0.87      0.85      0.84      2352\nweighted avg       0.87      0.84      0.84      2352\n\n🔵 Validation Loss: 0.1478, Accuracy: 95.92%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.99      0.96      0.98       490\n          TB       0.84      0.94      0.88        98\n\n    accuracy                           0.96       588\n   macro avg       0.91      0.95      0.93       588\nweighted avg       0.96      0.96      0.96       588\n\n✅ Model improved & saved!\n🟢 Epoch [2/10] - Training Loss: 0.0720\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.98      0.92      0.95      1168\n          TB       0.93      0.98      0.95      1184\n\n    accuracy                           0.95      2352\n   macro avg       0.96      0.95      0.95      2352\nweighted avg       0.96      0.95      0.95      2352\n\n🔵 Validation Loss: 0.0633, Accuracy: 98.13%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.98      0.99       490\n          TB       0.91      0.98      0.95        98\n\n    accuracy                           0.98       588\n   macro avg       0.96      0.98      0.97       588\nweighted avg       0.98      0.98      0.98       588\n\n✅ Model improved & saved!\n🟢 Epoch [3/10] - Training Loss: 0.0446\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.99      0.95      0.97      1148\n          TB       0.95      0.99      0.97      1204\n\n    accuracy                           0.97      2352\n   macro avg       0.97      0.97      0.97      2352\nweighted avg       0.97      0.97      0.97      2352\n\n🔵 Validation Loss: 0.1586, Accuracy: 95.41%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.98      0.97      0.97       490\n          TB       0.85      0.88      0.86        98\n\n    accuracy                           0.95       588\n   macro avg       0.91      0.92      0.92       588\nweighted avg       0.95      0.95      0.95       588\n\n⚠️ Early stopping counter: 1/5\n🟢 Epoch [4/10] - Training Loss: 0.0595\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.99      0.95      0.97      1194\n          TB       0.95      0.99      0.97      1158\n\n    accuracy                           0.97      2352\n   macro avg       0.97      0.97      0.97      2352\nweighted avg       0.97      0.97      0.97      2352\n\n🔵 Validation Loss: 0.0545, Accuracy: 97.62%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.98      0.99       490\n          TB       0.89      0.98      0.93        98\n\n    accuracy                           0.98       588\n   macro avg       0.94      0.98      0.96       588\nweighted avg       0.98      0.98      0.98       588\n\n✅ Model improved & saved!\n🟢 Epoch [5/10] - Training Loss: 0.0269\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.99      0.97      0.98      1201\n          TB       0.97      0.99      0.98      1151\n\n    accuracy                           0.98      2352\n   macro avg       0.98      0.98      0.98      2352\nweighted avg       0.98      0.98      0.98      2352\n\n🔵 Validation Loss: 0.0509, Accuracy: 96.77%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.96      0.98       490\n          TB       0.84      1.00      0.91        98\n\n    accuracy                           0.97       588\n   macro avg       0.92      0.98      0.95       588\nweighted avg       0.97      0.97      0.97       588\n\n✅ Model improved & saved!\n🟢 Epoch [6/10] - Training Loss: 0.0227\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.98      0.99      1133\n          TB       0.98      1.00      0.99      1219\n\n    accuracy                           0.99      2352\n   macro avg       0.99      0.99      0.99      2352\nweighted avg       0.99      0.99      0.99      2352\n\n🔵 Validation Loss: 0.0301, Accuracy: 98.13%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.98      0.99       490\n          TB       0.90      1.00      0.95        98\n\n    accuracy                           0.98       588\n   macro avg       0.95      0.99      0.97       588\nweighted avg       0.98      0.98      0.98       588\n\n✅ Model improved & saved!\n🟢 Epoch [7/10] - Training Loss: 0.0286\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.98      0.99      1191\n          TB       0.98      1.00      0.99      1161\n\n    accuracy                           0.99      2352\n   macro avg       0.99      0.99      0.99      2352\nweighted avg       0.99      0.99      0.99      2352\n\n🔵 Validation Loss: 0.0596, Accuracy: 97.45%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.97      0.98       490\n          TB       0.87      0.99      0.93        98\n\n    accuracy                           0.97       588\n   macro avg       0.94      0.98      0.96       588\nweighted avg       0.98      0.97      0.98       588\n\n⚠️ Early stopping counter: 1/5\n🟢 Epoch [8/10] - Training Loss: 0.0320\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.99      0.96      0.98      1166\n          TB       0.97      0.99      0.98      1186\n\n    accuracy                           0.98      2352\n   macro avg       0.98      0.98      0.98      2352\nweighted avg       0.98      0.98      0.98      2352\n\n🔵 Validation Loss: 0.0632, Accuracy: 98.30%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.99      0.99      0.99       490\n          TB       0.93      0.97      0.95        98\n\n    accuracy                           0.98       588\n   macro avg       0.96      0.98      0.97       588\nweighted avg       0.98      0.98      0.98       588\n\n✅ Model improved & saved!\n🟢 Epoch [9/10] - Training Loss: 0.0190\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.98      0.99      1166\n          TB       0.98      1.00      0.99      1186\n\n    accuracy                           0.99      2352\n   macro avg       0.99      0.99      0.99      2352\nweighted avg       0.99      0.99      0.99      2352\n\n🔵 Validation Loss: 0.0398, Accuracy: 98.30%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.99      0.99      0.99       490\n          TB       0.93      0.97      0.95        98\n\n    accuracy                           0.98       588\n   macro avg       0.96      0.98      0.97       588\nweighted avg       0.98      0.98      0.98       588\n\n⚠️ Early stopping counter: 1/5\n🟢 Epoch [10/10] - Training Loss: 0.0193\n📊 Training Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.98      0.99      1153\n          TB       0.98      1.00      0.99      1199\n\n    accuracy                           0.99      2352\n   macro avg       0.99      0.99      0.99      2352\nweighted avg       0.99      0.99      0.99      2352\n\n🔵 Validation Loss: 0.0810, Accuracy: 98.64%\n📊 Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.99      0.99      0.99       490\n          TB       0.97      0.95      0.96        98\n\n    accuracy                           0.99       588\n   macro avg       0.98      0.97      0.98       588\nweighted avg       0.99      0.99      0.99       588\n\n✅ Model improved & saved!\n✅ Training Complete!\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import classification_report\n\n# ✅ Load the Best Model\nmodel.load_state_dict(torch.load(\"efficientnet_vit_tb_best.pth\"))\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode\nprint(\"✅ Best model loaded successfully!\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:32:38.473680Z","iopub.execute_input":"2025-04-03T06:32:38.474018Z","iopub.status.idle":"2025-04-03T06:32:38.875797Z","shell.execute_reply.started":"2025-04-03T06:32:38.473991Z","shell.execute_reply":"2025-04-03T06:32:38.874902Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-29-c41d9e2c26d4>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"efficientnet_vit_tb_best.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"✅ Best model loaded successfully!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"val_preds, val_labels = [], []\n\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n\n        _, predicted = torch.max(outputs, 1)\n        val_preds.extend(predicted.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# ✅ Print Final Validation Metrics\nprint(\"\\n📊 Final Validation Classification Report:\")\nprint(classification_report(val_labels, val_preds, target_names=[\"Normal\", \"TB\"], zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:32:42.197864Z","iopub.execute_input":"2025-04-03T06:32:42.198195Z","iopub.status.idle":"2025-04-03T06:32:55.061938Z","shell.execute_reply.started":"2025-04-03T06:32:42.198166Z","shell.execute_reply":"2025-04-03T06:32:55.061140Z"}},"outputs":[{"name":"stdout","text":"\n📊 Final Validation Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.99      0.99      0.99       490\n          TB       0.97      0.94      0.95        98\n\n    accuracy                           0.98       588\n   macro avg       0.98      0.97      0.97       588\nweighted avg       0.98      0.98      0.98       588\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# ✅ Load Test Dataset\ntest_dataset = TuberculosisDataset(test_df, test_image_folder, transform=test_transform, is_test=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n# ✅ Make Predictions with TTA\nall_preds = []\nall_ids = []\n\nfor images, img_names in test_loader:\n    images = images.to(device)  # Move to GPU/CPU\n    tta_pred = tta_inference(model, images, device)  # Apply TTA\n    predicted_class = np.argmax(tta_pred, axis=1)  # Convert probability to label\n    \n    all_preds.extend(predicted_class)\n    all_ids.extend(img_names)\n\n# ✅ Save Predictions for Kaggle Submission\nsubmission = pd.DataFrame({\"id\": all_ids, \"label\": all_preds})\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:33:11.943514Z","iopub.execute_input":"2025-04-03T06:33:11.943924Z","iopub.status.idle":"2025-04-03T06:33:11.973655Z","shell.execute_reply.started":"2025-04-03T06:33:11.943886Z","shell.execute_reply":"2025-04-03T06:33:11.971110Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-36d4425c0775>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ✅ Load Test Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTuberculosisDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ✅ Make Predictions with TTA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_transform' is not defined"],"ename":"NameError","evalue":"name 'test_transform' is not defined","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"# import torch\n# import torchvision.transforms as transforms\n# import numpy as np\n# from PIL import Image\n\n# def tta_inference(model, image_tensor, device, tta_transforms=5):\n#     \"\"\"Perform TTA by averaging predictions over multiple augmented versions of the image.\"\"\"\n#     model.eval()\n\n#     # ✅ Convert `torch.Tensor` to `PIL Image`\n#     unnormalize = transforms.Normalize(mean=[-0.5 / 0.5], std=[1 / 0.5])  # Undo normalization\n#     image_tensor = unnormalize(image_tensor.squeeze(0))  # Remove batch dimension\n#     image_np = image_tensor.cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC NumPy array\n#     image_pil = Image.fromarray((image_np * 255).astype(np.uint8))  # Convert to PIL Image\n\n#     # ✅ Define minor augmentations for TTA\n#     tta_transform = transforms.Compose([\n#         transforms.RandomHorizontalFlip(p=0.5),\n#         transforms.RandomRotation(degrees=10),\n#         transforms.ColorJitter(brightness=0.1, contrast=0.1),\n#         transforms.Resize((224, 224)), \n#         transforms.ToTensor(),\n#         transforms.Normalize(mean=[0.5], std=[0.5])\n#     ])\n\n#     predictions = []\n#     with torch.no_grad():\n#         for _ in range(tta_transforms):\n#             aug_image = tta_transform(image_pil)  # Apply small transformation\n#             aug_image = aug_image.unsqueeze(0).to(device)  # Add batch dimension\n#             output = model(aug_image)  \n#             predictions.append(torch.softmax(output, dim=1).cpu().numpy())  # Convert to probability\n\n#     final_pred = np.mean(predictions, axis=0)  # Average the predictions\n#     return final_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T00:16:16.910665Z","iopub.status.idle":"2025-03-30T00:16:16.910980Z","shell.execute_reply":"2025-03-30T00:16:16.910818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# ✅ Define test folder path\ntest_image_folder = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Test_Dataset_TB\"\n\n# ✅ Get list of test images\ntest_images = os.listdir(test_image_folder)\n\n# ✅ Create `test_df` with image names (without labels)\ntest_df = pd.DataFrame({\"image_name\": test_images, \"label\": -1})  \n\nprint(f\"✅ Test DataFrame Shape: {test_df.shape}\")\nprint(test_df.head())  # Check first few rows\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:33:24.790485Z","iopub.execute_input":"2025-04-03T06:33:24.790776Z","iopub.status.idle":"2025-04-03T06:33:24.799863Z","shell.execute_reply.started":"2025-04-03T06:33:24.790753Z","shell.execute_reply":"2025-04-03T06:33:24.798995Z"}},"outputs":[{"name":"stdout","text":"✅ Test DataFrame Shape: (1260, 2)\n         image_name  label\n0  TB_test_0370.png     -1\n1  TB_test_0419.png     -1\n2  TB_test_0521.png     -1\n3  TB_test_0653.png     -1\n4  TB_test_0722.png     -1\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# import pandas as pd\n\n# # ✅ Load the submission file\n# submission_df = pd.read_csv(\"submission.csv\")\n\n# # ✅ Check the number of rows\n# print(\"Submission CSV Shape:\", submission_df.shape)  # Should be (1260, 2)\n\n# # ✅ Display the first few rows\n# print(submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T00:16:16.912892Z","iopub.status.idle":"2025-03-30T00:16:16.913156Z","shell.execute_reply":"2025-03-30T00:16:16.913044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Define Tuberculosis Dataset Class\nclass TuberculosisDataset(Dataset):\n    def __init__(self, data, image_folder, transform=None, is_test=False):\n        self.data = data\n        self.image_folder = image_folder\n        self.transform = transform\n        self.is_test = is_test  # ✅ Add flag for test dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = self.data.iloc[idx, 0]  # ✅ First column should be image name\n        img_path = os.path.join(self.image_folder, img_name)\n\n        if not os.path.exists(img_path):\n            print(f\"⚠️ Warning: {img_name} not found! Skipping...\")\n            return self.__getitem__((idx + 1) % len(self.data))  # Skip to next image\n\n        image = Image.open(img_path).convert(\"RGB\")  # Open image\n        if self.transform:\n            image = self.transform(image)\n\n        if self.is_test:\n            return image, img_name  # ✅ Return image name for submission\n        else:\n            label = self.data.iloc[idx, 1]  # ✅ Second column should be label\n            return image, torch.tensor(label, dtype=torch.long)\n\nprint(\"✅ Dataset class updated successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:33:29.710444Z","iopub.execute_input":"2025-04-03T06:33:29.710728Z","iopub.status.idle":"2025-04-03T06:33:29.717770Z","shell.execute_reply.started":"2025-04-03T06:33:29.710707Z","shell.execute_reply":"2025-04-03T06:33:29.716644Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset class updated successfully!\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"test_dataset = TuberculosisDataset(test_df, test_image_folder, transform=test_transform, is_test=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\nprint(f\"✅ Test dataset loaded: {len(test_dataset)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:33:33.756506Z","iopub.execute_input":"2025-04-03T06:33:33.756807Z","iopub.status.idle":"2025-04-03T06:33:33.774588Z","shell.execute_reply.started":"2025-04-03T06:33:33.756783Z","shell.execute_reply":"2025-04-03T06:33:33.773475Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-fa6ff35bb986>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTuberculosisDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Test dataset loaded: {len(test_dataset)} samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_transform' is not defined"],"ename":"NameError","evalue":"name 'test_transform' is not defined","output_type":"error"}],"execution_count":35},{"cell_type":"code","source":"# ✅ Run Model Inference on Test Data\npredictions = []\n\nwith torch.no_grad():\n    for images, img_names in test_loader:  # ✅ Ensure img_names are correctly retrieved\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n\n        # ✅ Append results with correct ID\n        for name, pred in zip(img_names, predicted.cpu().numpy()):\n            predictions.append([name, pred])\n\n# ✅ Convert to DataFrame\nsubmission_df = pd.DataFrame(predictions, columns=[\"ID\", \"Target\"])\n\n# ✅ Save CSV File\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Submission file saved as 'submission.csv'!\")\n\n\nprint(\"✅ Sample Submission:\")\nprint(submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:33:42.741782Z","iopub.execute_input":"2025-04-03T06:33:42.742109Z","iopub.status.idle":"2025-04-03T06:33:57.527493Z","shell.execute_reply.started":"2025-04-03T06:33:42.742081Z","shell.execute_reply":"2025-04-03T06:33:57.526492Z"}},"outputs":[{"name":"stdout","text":"✅ Submission file saved as 'submission.csv'!\n✅ Sample Submission:\n                 ID  Target\n0  TB_test_0370.png       0\n1  TB_test_0419.png       0\n2  TB_test_0521.png       0\n3  TB_test_0653.png       0\n4  TB_test_0722.png       1\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# # ✅ Define Test Image Folder Path\n# test_image_folder = \"/kaggle/input/tb-data/tbdata/Test_Dataset_TB\"\n# image_folder = \"/kaggle/input/tb-data/tbdata/Training_Dataset_TB\"\n\n# # ✅ Define Transformations (Same as Training Preprocessing)\n# test_transform = transforms.Compose([\n#     transforms.Resize((224, 224)), \n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.5], std=[0.5])\n# ])\n\n# # ✅ Create Test Dataset (Corrected)\n# test_dataset = TuberculosisDataset(test_image_folder,image_folder, transform=test_transform)\n\n# # ✅ Create Test DataLoader\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# print(f\"✅ Test dataset loaded: {len(test_dataset)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T00:16:16.917640Z","iopub.status.idle":"2025-03-30T00:16:16.917929Z","shell.execute_reply":"2025-03-30T00:16:16.917792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# ✅ Define Test Image Folder Path\ntest_image_folder = \"/kaggle/input/d/monamisomsaha/tuberculosis-detection-from-chest-x-rays/Finance/TB_Dataset/Test_Dataset_TB\"\n\n# ✅ Get List of Test Images\ntest_images = os.listdir(test_image_folder)\n\n# ✅ Create a DataFrame with Dummy Labels (-1 for test data)\ntest_df = pd.DataFrame({\"ID\": test_images, \"Target\": -1})\n\nprint(f\"✅ Created test DataFrame with {len(test_df)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:34:02.766967Z","iopub.execute_input":"2025-04-03T06:34:02.767323Z","iopub.status.idle":"2025-04-03T06:34:02.774994Z","shell.execute_reply.started":"2025-04-03T06:34:02.767293Z","shell.execute_reply":"2025-04-03T06:34:02.774153Z"}},"outputs":[{"name":"stdout","text":"✅ Created test DataFrame with 1260 samples\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# ✅ Create Test Dataset\ntest_dataset = TuberculosisDataset(test_df, test_image_folder, transform=test_transform)\n\n# ✅ Create Test DataLoader\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\nprint(f\"✅ Test dataset loaded: {len(test_dataset)} samples\")\nfor images, img_names in test_loader:\n    print(\"Batch Sample IDs:\", img_names[:10])  # Print first 10 image names\n    break  # Only check one batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:34:05.766880Z","iopub.execute_input":"2025-04-03T06:34:05.767199Z","iopub.status.idle":"2025-04-03T06:34:05.798429Z","shell.execute_reply.started":"2025-04-03T06:34:05.767171Z","shell.execute_reply":"2025-04-03T06:34:05.797143Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-46381cc9d5ab>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ✅ Create Test Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTuberculosisDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ✅ Create Test DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_transform' is not defined"],"ename":"NameError","evalue":"name 'test_transform' is not defined","output_type":"error"}],"execution_count":38},{"cell_type":"code","source":"model.eval()\ntest_preds, test_ids = [], []\n\nwith torch.no_grad():\n    for images, img_names in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n\n        test_preds.extend(predicted.cpu().numpy())  # Store predictions\n        test_ids.extend(img_names)  # Store filenames\n\n# ✅ Convert Predictions to DataFrame\nsubmission_df = pd.DataFrame({\"ID\": test_ids, \"Target\": test_preds})\n\n# ✅ Save Submission CSV\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Submission file saved as 'submission.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:34:08.670483Z","iopub.execute_input":"2025-04-03T06:34:08.670785Z","iopub.status.idle":"2025-04-03T06:34:21.836456Z","shell.execute_reply.started":"2025-04-03T06:34:08.670762Z","shell.execute_reply":"2025-04-03T06:34:21.835572Z"}},"outputs":[{"name":"stdout","text":"✅ Submission file saved as 'submission.csv'\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/working\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:34:33.259379Z","iopub.execute_input":"2025-04-03T06:34:33.259690Z","iopub.status.idle":"2025-04-03T06:34:33.264677Z","shell.execute_reply.started":"2025-04-03T06:34:33.259666Z","shell.execute_reply":"2025-04-03T06:34:33.263908Z"}},"outputs":[{"name":"stdout","text":"['submission.csv', 'TB_combined_labels.csv', 'efficientnet_vit_tb_best.pth', '.virtual_documents']\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"old code NOT NECESSARY so removed \n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}